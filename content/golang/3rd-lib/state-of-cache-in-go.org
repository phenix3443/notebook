# -*- coding:utf-8-*-
#+TITLE: state of cache in go
#+AUTHOR: liushangliang
#+EMAIL: phenix3443+github@gmail.com

* 概述
  本文主要介绍 golang 中的内存缓存库。原文 https://blog.dgraph.io/post/caching-in-go/

* 内存缓存要求
  1. 并发。
  2. 内存限制（可配置最大内存使用量）。
  3. 随着核心和 goroutine 数量的增加，扩展性也很好。
  4. 在非随机 key 访问分布（例如 Zipf）下可以很好地扩展。
  5. 高命中率。

*  Go map 和 sync.Mutex
  将 Go 映射与 sync.Mutex（或 sync.RWMutex）一起使用是最常用的缓存方法。但这会导致所有 goroutine 在获取锁的时候阻塞，从而导致竞争。这也无法限制内存使用量。

  不符合 2-5。(4-5 为什么不符合？)

* Go maps 和分离锁（lock striping）
  这与上面的概念相同，但是使用指纹将 key 划分为许多较小的，由互斥锁保护的 Go Map 碎片（请参见此处）。许多开发人员错误地认为分离锁是避免争用的好方法，尤其是在将分片的数量设置为超过程序中的线程数时（> GOMAXPROCS）。

  在我们最初尝试构建简化的内存缓存时，我们也构建了它。为了允许将内存释放回操作系统，我们将定期选择一个随机分片并删除其映射，以便重新填充。这是一种粗略但简单的技术，其性能优于 LRU 缓存（如下所述），但有很多缺点。

  第一，Go 将内存释放回操作系统的速度很慢，但是请求内存的速度却很快。一旦分片被清空，尝试访问该分片中的键的 goroutine 将开始分配内存，而先前的内存仍未完全释放，从而导致内存使用量激增和 OOM 迅速崩溃。

  另外，我们当时没有意识到的是，访问模式受 Zipf 定律的限制。最常访问的键仍处于少数锁中，因此成为所有 goroutine 争用的原因。这种方法的性能无法随内核数量很好地扩展。

  不符合 2 和 4。

* LRU cache
  Go 具有基本的 LRU 缓存实现，它是 groupcache 的一部分。在尝试使用映射和分离锁失败之后，我们通过引入锁来修改此 LRU 缓存以支持并发。尽管此缓存确实解决了之前由于频繁的内存释放而导致的内存高峰问题，但我们意识到它将引入竞争。

  此高速缓存的大小还取决于条目的数量，而不是它们消耗的内存量。在 Go 中尝试估计复杂数据结构在堆上的内存使用是非常昂贵的，几乎是不可能实现的，这是我们在使用多种机制徒劳尝试之后才意识到的。这特别困难，因为我们的数据结构在放入缓存后发生了变化（我们计划避免这种情况）。

  但是，我们不了解该缓存可能引起多少争用。使用此缓存一年后，我们意识到围绕此缓存的争用非常重大，以至于删除它会使我们的查询速度提高 10 倍！

  在此实现中，每次读取都是写操作，用来更新元素在最近度链接列表中的相对位置。因此，所有访问都等待单个互斥锁。此外，LRU 的关键部分比映射慢，并且会进行很多指针取消引用，从而维护映射和双向链接列表（请参见代码）。尽管我们在懒惰驱逐方面做出了努力，但仍然存在严重的争议。

  未能达到要求 3-4。

* LRU cache 和分离锁
  我们没有尝试这样做。从 map 和分离锁的试验中，我们知道这只会是部分的改进，并且无法很好地扩展。 （不过，出于对本文中的缓存进行基准测试的目的，我们实现了如下所述的条带化 LRU 缓存。）

  将无法满足要求 4。

* 当前流行的 cache 实现方案
  许多其他方法旨在减少花费在 map 碎片上的 GC 时间。 GC 时间随着映射中条目数量的增加而增加。通过分配更少，更大的字节片并在每个片中存储许多缓存条目来实现减少 map 的碎皮哦按回收。这是一种有效的方法-我们在 Badger 中的多个位置（“跳过列表”，“表构建器”等）中使用此方法。 Go 中一些流行的缓存使用此技术。

** BigCache
   BigCache 根据 key 的哈希将数据分为碎片。每个分片都包含一个映射和一个环形缓冲区。每当设置新元素时，它都会将该元素追加到相应分片的环形缓冲区中，并且缓冲区中的偏移量将存储在映射中。如果同一元素被设置多次，则缓冲区中的先前条目将标记为无效。如果缓冲区太小，则将其扩展直到达到最大容量。

   每个 map key 都是一个 uint32 哈希，其值是一个 uint32 指针，指向该值与元数据信息一起存储的缓冲区中的偏移量。如果存在哈希冲突，则 BigCache 会忽略前一个键并将当前键存储到 map 中。预先分配较少，较大的缓冲区并使用 map [uint32] uint32 是避免 GC 扫描成本的好方法。

** FreeCache
   FreeCache 将缓存分为 256 个段。每个段包含 256 个插槽和一个环形缓冲区以存储数据。将新 key 添加到高速缓存时，将使用 key 哈希的低八位来标识段 ID。此外，使用 key 的哈希的 LSB 9-16 选择一个插槽。将数据划分为多个插槽有助于减少在缓存中查找键时的搜索空间。

   然后将数据附加到环形缓冲区中，并将偏移量存储到 *排序数组* 中。如果环形缓冲区没有足够的空间，则使用修改后的 LRU 策略从环形缓冲区的开头开始在该段中删除。如果该条目的最后访问时间小于段的平均访问时间，则从环形缓冲区中删除该条目。要在高速缓存中查找条目，在相应插槽中的排序数组中执行二进制搜索。

** GroupCache
   GroupCache 使用链接列表和 Go 映射实现确切的 LRU 逐出策略。为了公平地比较，我们在 GroupCache 的顶部实现了具有 256 个分片的分片逻辑。

** compare

*** Read-Only

    鉴于读取是无锁的，因此我们可以看到 BigCache 读取扩展良好。 FreeCache 和 GroupCache 读取不是无锁的，并且在一个点（20 个并发访问）之后不会扩展。 （值越高，y轴越好）

*** Write-Only

    对于只写工作负载，所有库似乎都具有类似的性能。不过，FreeCache 的性能比其他两个要好一些。

*** Read-Write (25% writes, 75% reads)

    对于包含 25％的写入和 75％的读取的混合工作负载，虽然 BigCache 是​​​​唯一显然可以很好地扩展的库，但是命中率对于 Zipf 工作负载不利，如下一节所述。

*** 命中率比较
    三个缓存的命中率如下所示。 FreeCache 与 GroupCache 实施的 LRU 策略非常接近。但是，由于以下原因，BigCache 不适用于 Zipf 分布式工作负载：
    + BigCache 无法有效利用缓冲区，最终可能会在缓冲区中存储同一键的多个条目。
    + BigCache 不会更新访问（读取）上的条目，因此会导致驱逐最近访问的密钥。


    因此，我们可以得出结论，没有一个缓存库可以满足所有要求。 GroupCache 和 FreeCache 在要求 4 上失败，而 BigCache 在要求 5 上失败。

* 结论
 最终，我们还没有找到到 Go 中可以满足全部需求列表的智能内存绑定缓存。

 同时，我们遇到了 Caffeine，这是 Cassandra，Finagle 和其他数据库系统使用的 Java 库。它使用 [[https://arxiv.org/abs/1512.00727][TinyLFU]]（一种高效的高速缓存允许策略），并使用各种技术来随着线程和核心数量的增长而扩展和良好地执行，同时提供接近最佳的命中率。[[https://docs.google.com/presentation/d/1NlDxyXsUG1qlVHMl4vsUUBQfAJ2c2NsFPNPr2qymIBs/edit?usp=sharing][（perform）]] 可以在[[http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html][本文]]中了解有关其工作原理的更多信息。

 Caffeine 满足了我在开始时提到的所有五个要求，因此我们正在考虑构建等效于 Go 的 Caffeine，它可以满足我们的需求，并有可能填补 Go 语言中并发，高性能，内存受限的缓存的空白。

 By:最后他们自己造了一个轮子： [[https://blog.dgraph.io/post/introducing-ristretto-high-perf-go-cache/][Introducing Ristretto: A High-Performance Go Cache]]
